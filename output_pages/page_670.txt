Section 6.5
Writing Cache-Friendly Code
669
Impact of Block Size
Large blocks are a mixed blessing. On the one hand, larger blocks can help
increase the hit rate by exploiting any spatial locality that might exist in a program.
However, for a given cache size, larger blocks imply a smaller number of cache
lines, which can hurt the hit rate in programs with more temporal locality than
spatial locality. Larger blocks also have a negative impact on the miss penalty,
since larger blocks cause larger transfer times. Modern systems such as the Core
i7 compromise with cache blocks that contain 64 bytes.
Impact of Associativity
The issue here is the impact of the choice of the parameter E, the number of
cache lines per set. The advantage of higher associativity (i.e., larger values of E)
is that it decreases the vulnerability of the cache to thrashing due to conﬂict misses.
However, higher associativity comes at a signiﬁcant cost. Higher associativity is
expensive to implement and hard to make fast. It requires more tag bits per
line, additional LRU state bits per line, and additional control logic. Higher
associativity can increase hit time, because of the increased complexity, and it can
also increase the miss penalty because of the increased complexity of choosing a
victim line.
The choice of associativity ultimately boils down to a trade-off between the
hit time and the miss penalty. Traditionally, high-performance systems that pushed
the clock rates would opt for smaller associativity for L1 caches (where the miss
penalty is only a few cycles) and a higher degree of associativity for the lower
levels, where the miss penalty is higher. For example, in Intel Core i7 systems, the
L1 and L2 caches are 8-way associative, and the L3 cache is 16-way.
Impact of Write Strategy
Write-through caches are simpler to implement and can use a write buffer that
works independently of the cache to update memory. Furthermore, read misses
are less expensive because they do not trigger a memory write. On the other
hand, write-back caches result in fewer transfers, which allows more bandwidth
to memory for I/O devices that perform DMA. Further, reducing the number of
transfers becomes increasingly important as we move down the hierarchy and the
transfer times increase. In general, caches further down the hierarchy are more
likely to use write-back than write-through.
6.5
Writing Cache-Friendly Code
In Section 6.2, we introduced the idea of locality and talked in qualitative terms
about what constitutes good locality. Now that we understand how cache memo-
ries work, we can be more precise. Programs with better locality will tend to have
lower miss rates, and programs with lower miss rates will tend to run faster than
programs with higher miss rates. Thus, good programmers should always try to
