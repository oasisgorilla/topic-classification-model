534
Chapter 5
Optimizing Program Performance
can also predict what operations will be performed in parallel and how well they
will use the processor resources. As we will see, we can often determine the time
(or at least a lower bound on the time) required to execute a loop by identifying
critical paths, chains of data dependencies that form during repeated executions
of a loop. We can then go back and modify the source code to try to steer the
compiler toward more efﬁcient implementations.
Most major compilers, including gcc, are continually being updated and im-
proved, especially in terms of their optimization abilities. One useful strategy is to
do only as much rewriting of a program as is required to get it to the point where
the compiler can then generate efﬁcient code. By this means, we avoid compro-
mising the readability, modularity, and portability of the code as much as if we had
to work with a compiler of only minimal capabilities. Again, it helps to iteratively
modify the code and analyze its performance both through measurements and by
examining the generated assembly code.
To novice programmers, it might seem strange to keep modifying the source
code in an attempt to coax the compiler into generating efﬁcient code, but this
is indeed how many high-performance programs are written. Compared to the
alternative of writing code in assembly language, this indirect approach has the
advantage that the resulting code will still run on other machines, although per-
haps not with peak performance.
5.1
Capabilities and Limitations of Optimizing Compilers
Modern compilers employ sophisticated algorithms to determine what values are
computed in a program and how they are used. They can then exploit opportuni-
ties to simplify expressions, to use a single computation in several different places,
and to reduce the number of times a given computation must be performed. Most
compilers, including gcc, provide users with some control over which optimiza-
tions they apply. As discussed in Chapter 3, the simplest control is to specify the
optimization level. For example, invoking gcc with the command-line option -Og
speciﬁes that it should apply a basic set of optimizations.
Invoking gcc with option -O1 or higher (e.g., -O2 or -O3) will cause it to apply
more extensive optimizations. These can further improve program performance,
but they may expand the program size and they may make the program more
difﬁcult to debug using standard debugging tools. For our presentation, we will
mostly consider code compiled with optimization level -O1, even though level
-O2 has become the accepted standard for most software projects that use gcc.
We purposely limit the level of optimization to demonstrate how different ways
of writing a function in C can affect the efﬁciency of the code generated by a
compiler. We will ﬁnd that we can write C code that, when compiled just with
option -O1, vastly outperforms a more naive version compiled with the highest
possible optimization levels.
Compilers must be careful to apply only safe optimizations to a program,
meaning that the resulting program will have the exact same behavior as would
an unoptimized version for all possible cases the program may encounter, up to
the limits of the guarantees provided by the C language standards. Constraining
